name: AXLearn GKE H100 flash attention test
on:
  schedule:
  - cron: 17 0 * * *
  workflow_dispatch:

jobs:
  axlearn-flash-attention-h100:
    # You need to use the INSTALLATION_NAME from the previous step
    runs-on: arc-runner-h100
    env:
      PIP_FIND_LINKS: "https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      LD_LIBRARY_PATH: "/usr/local/nvidia/lib64"
    steps:
      - run: ls /usr/local/nvidia/lib64
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      - run: pip install --upgrade pip
      - run: pip install '.[core,gcp,gpu]'
      # Pin specific Jax version
      - run: pip install --upgrade --force-reinstall 'jax[cuda12]==0.5.3'
      - run: pip install 'pytest'
      - run: pytest axlearn/common/flash_attention/gpu_attention_test.py
