name: AXLearn GKE H100 flash attention test
on:
  schedule:
  - cron: 17 0 * * *
  workflow_dispatch:
    inputs:
      jax_version:
        required: true
        default: '0.5.1'
        type: string
jobs:
  axlearn-flash-attention-h100:
    # You need to use the INSTALLATION_NAME from the previous step
    runs-on: arc-runner-h100
    env:
      PIP_FIND_LINKS: "https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      LD_LIBRARY_PATH: "/usr/local/nvidia/lib64"
      JAX_VERSION: ${{ inputs.jax_version }}
    steps:
      - run: ls /usr/local/nvidia/lib64
      - run: ls /usr/local/nvidia/bin
      - run: echo "/usr/local/nvidia/bin" >> "$GITHUB_PATH"
      - run: nvidia-smi
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      - run: pip install --upgrade pip
      - run: pip install '.[core,gcp,gpu]'
      # Pin specific Jax version
      - run: pip install --upgrade --force-reinstall "jax[cuda12]==${JAX_VERSION}"
      - run: pip install 'pytest'
      - run: pytest axlearn/common/flash_attention/gpu_attention_test.py
